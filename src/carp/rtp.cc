#include "carp/rtp.h"

#include "msgfmt.h"
#include "preload_internal.h"

namespace {

/* convert double array to string (only used for printing in debug log) */
std::string darr2str(double *v, size_t vsz) {
  std::string rv;
  for (size_t i = 0 ; i < vsz ; i++) {
    if (i == 0) {
      rv = std::to_string(v[i]);
    } else {
      rv += "," + std::to_string(v[i]);
    }
  }
  return rv;
}

/* Hash function, from
 * https://stackoverflow.com/questions/8317508/hash-function-for-a-string
 */
#define A 54059   /* a prime */
#define B 76963   /* another prime */
#define C 86969   /* yet another prime */
#define FIRSTH 37 /* also prime */

uint32_t hash_str(const char* data, int slen) {
  uint32_t h = FIRSTH;
  for (int sidx = 0; sidx < slen; sidx++) {
    char s = data[sidx];
    h = (h * A) ^ (s * B);
  }
  return h % C;
}

}  // namespace

namespace pdlfs {
namespace carp {
RTP::RTP(Carp* carp, const CarpOptions& opts)
    : carp_(carp),
      sh_(nullptr),
      pivot_buffer_(opts.rtp_pvtcnt),
      round_num_(0),
      my_rank_(opts.my_rank),
      num_ranks_(opts.num_ranks) {
  if (state_.GetState() != RenegState::INIT) {
    flog(LOG_ERRO, "rtp_init: can initialize only in init stage");
    return;
  }

  if (opts.sctx->type != SHUFFLE_XN) {
    flog(LOG_ERRO, "Only 3-hop is supported by the RTP protocol");
    return;
  }

  if (opts.rtp_pvtcnt[1] > CARP_MAXPIVOTS) {
    flog(LOG_ERRO, "pvtcnt_s1 exceeds MAX_PIVOTS");
    return;
  }

  if (opts.rtp_pvtcnt[2] > CARP_MAXPIVOTS) {
    flog(LOG_ERRO, "pvtcnt_s1 exceeds MAX_PIVOTS");
    return;
  }

  if (opts.rtp_pvtcnt[3] > CARP_MAXPIVOTS) {
    flog(LOG_ERRO, "pvtcnt_s1 exceeds MAX_PIVOTS");
    return;
  }

  sh_ = static_cast<xn_ctx_t*>(opts.sctx->rep);

  //  rctx->carp = carp;

  /* Number of pivots generated by each stage */
  pvtcnt_[1] = opts.rtp_pvtcnt[1];
  pvtcnt_[2] = opts.rtp_pvtcnt[2];
  pvtcnt_[3] = opts.rtp_pvtcnt[3];

  if (pctx.my_rank == 0) {
    flog(LOG_INFO, "[rtp_init] pivot_count: %d/%d/%d", pvtcnt_[1], pvtcnt_[2],
         pvtcnt_[3]);
  }

  Status s = InitTopology();

  if (s.ok()) {
    state_.UpdateState(RenegState::READY);
  }
}

Status RTP::InitRound() {
  Status s = Status::OK();

  carp_->AssertLockHeld();
  mutex_.Lock();

  if (state_.GetState() == RenegState::READY) {
    s = BroadcastBegin();
    state_.UpdateState(RenegState::READYBLOCK);
    carp_->UpdateState(MainThreadState::MT_READYBLOCK);
  }

  mutex_.Unlock();

  while (carp_->GetCurState() != MainThreadState::MT_READY) {
    carp_->CVWait();
  }

  reneg_bench_.MarkFinished();

  return s;
}

Status RTP::HandleMessage(void* buf, unsigned int bufsz, int src,
                          uint32_t type) {
  Status s = Status::OK();
  switch (type) {
    case MSGFMT_RTP_BEGIN:
      s = HandleBegin(buf, bufsz, src);
      break;
    case MSGFMT_RTP_PIVOT:
      s = HandlePivots(buf, bufsz, src);
      break;
    case MSGFMT_RTP_PVT_BCAST:
      s = HandlePivotBroadcast(buf, bufsz, src);
      break;
    default:
      ABORT("rtp_handle_message: unknown msg_type");
      break;
  }

  return s;
}

int RTP::NumRounds() const { return round_num_; }

// --------- PRIVATE METHODS ---------

Status RTP::InitTopology() {
  Status s = Status::OK();

  if (state_.GetState() != RenegState::INIT) {
    flog(LOG_DBUG, "rtp_topology_init: can initialize only in init stage");
    return Status::AssertionFailed("RTP: Not in INIT state");
  }

  int grank, gsz;
  grank = my_rank_;
  gsz = num_ranks_;

  RTPUtil::ComputeTreeFanout(gsz, fanout_);
  if (my_rank_ == 0)
    flog(LOG_INFO, "RTP Fanout: %d/%d/%d", fanout_[1], fanout_[2], fanout_[3]);

  /* Init peers for each stage */
  int s1mask = ~(fanout_[1] - 1);
  int s2mask = ~(fanout_[1] * fanout_[2] - 1);
  int s3mask = ~((fanout_[1] * fanout_[2] * fanout_[3]) - 1);

  int s1root = grank & s1mask;
  root_[1] = s1root;

  for (int pidx = 0; pidx < fanout_[1]; pidx++) {
    int jump = 1;
    int peer = s1root + pidx * jump;

    if (peer >= gsz) break;

    peers_[1][pidx] = peer;
  }

  int s2root = grank & s2mask;
  root_[2] = s2root;

  for (int pidx = 0; pidx < fanout_[2]; pidx++) {
    int jump = fanout_[1];
    int peer = s2root + pidx * jump;

    if (peer >= gsz) break;

    peers_[2][pidx] = peer;
  }

  int s3root = grank & s3mask;
  root_[3] = s3root;

  for (int pidx = 0; pidx < fanout_[3]; pidx++) {
    int jump = fanout_[1] * fanout_[2];
    int peer = s3root + pidx * jump;

    if (peer >= gsz) break;

    peers_[3][pidx] = peer;
  }

  return s;
}

Status RTP::BroadcastBegin() {
  Status s = Status::OK();
  mutex_.AssertHeld();

  reneg_bench_.MarkStart();

  flog(LOG_DBUG, "broadcast_rtp_begin: at rank %d, to %d", my_rank_, root_[3]);

  char buf[256];
  int buflen = msgfmt_encode_rtp_begin(buf, 256, my_rank_, round_num_);
  SendToRank(buf, buflen, root_[3], MSGFMT_RTP_BEGIN);

  return s;
}

Status RTP::SendToRank(const void* buf, int bufsz, int rank, uint32_t type) {
  xn_shuffle_priority_send(sh_, (void*)buf, bufsz, 0 /*XXXCDC remove epoch?*/,
                           rank, my_rank_, type);
  return Status::OK();
}

Status RTP::SendToAll(int stage, const void* buf, int bufsz, bool exclude_self,
                      uint32_t type) {
  for (int rank_idx = 0; rank_idx < fanout_[stage]; rank_idx++) {
    int drank = peers_[stage][rank_idx];

    /*  Not enabled by default; my_rank is set to -1 unless overridden */
    if (drank == my_rank_ && exclude_self) continue;

    flog(LOG_DBUG, "send_to_all: sending from %d to %d", my_rank_, drank);

    xn_shuffle_priority_send(sh_, (void*)buf, bufsz, 0 /*XXXCDC rm epoch?*/,
                             drank, my_rank_, type);
  }

  return Status::OK();
}

Status RTP::SendToChildren(const void* buf, int bufsz, bool exclude_self,
                           uint32_t type) {
  Status s = Status::OK();

#define AMROOT(x) (my_rank_ == root_[(x)])

  if (AMROOT(3)) {
    SendToAll(3, buf, bufsz, exclude_self, type);
  }
  if (AMROOT(2)) {
    SendToAll(2, buf, bufsz, exclude_self, type);
  }
  if (AMROOT(1)) {
    SendToAll(1, buf, bufsz, exclude_self, type);
  }

#undef AMROOT

  return s;
}

Status RTP::HandleBegin(void* buf, unsigned int bufsz, int src) {
  Status s = Status::OK();

  int srank, msg_round_num;
  msgfmt_decode_rtp_begin(buf, bufsz, &srank, &msg_round_num);

  flog(LOG_DBUG, "Received RTP_BEGIN_%d at Rank %d from %d", msg_round_num,
       my_rank_, src);

  bool activated_now = false;

  carp_->Lock();
  mutex_.Lock();

  if (msg_round_num < round_num_) {
    flog(LOG_DBG2,
         "Stale RENEG_BEGIN received at Rank %d, dropping"
         "(theirs: %d, ours: %d)",
         my_rank_, msg_round_num, round_num_);
  } else if (state_.GetState() == RenegState::READY ||
             state_.GetState() == RenegState::READYBLOCK) {
    /* If we're ready for Round R, no way we can receive R+1 first */
    assert(msg_round_num == round_num_);
    carp_->UpdateState(MainThreadState::MT_BLOCK);
    state_.UpdateState(RenegState::PVTSND);
    reneg_bench_.MarkActive();
    activated_now = true;
  } else if (msg_round_num == round_num_ + 1) {
    state_.MarkNextRoundStart(msg_round_num);
  } else if (msg_round_num != round_num_) {
    /* If round_nums are equal, msg is duplicate and DROP */
    ABORT("rtp_handle_rtp_begin: unexpected msg_round_num recvd");
  } else {
    assert(msg_round_num == round_num_);
  }

  mutex_.Unlock();
  carp_->Unlock();

  if (activated_now) {
    /* Can reuse the same RTP_BEGIN buf. src_rank is anyway sent separately
     * If we're an S3 root, prioritize Stage 3 sending first, so as to
     * trigger other leaf broadcasts in parallel
     * No need to send it to ourself since we just got it....
     * */
    SendToChildren(buf, bufsz, /* exclude_self */ true, MSGFMT_RTP_BEGIN);

    /* send pivots to s1root now */
    const int stage_idx = 1;

    carp_->Lock();

    int pvtcnt = pvtcnt_[stage_idx];

    int pvt_buf_sz = msgfmt_bufsize_rtp_pivots(pvtcnt);
    char pvt_buf[pvt_buf_sz];
    int pvt_buf_len;
    Pivots initial_pivots(pvtcnt);

    carp_->CalculatePivots(initial_pivots);
    carp_->LogPivots(initial_pivots);

    pvt_buf_len = msgfmt_encode_rtp_pivots(pvt_buf, pvt_buf_sz, round_num_,
                                           stage_idx, my_rank_,
                                           initial_pivots.Data(),
                                           initial_pivots.PivotWeight(),
                                           initial_pivots.Size());

    carp_->Unlock();

    flog(LOG_DBG2, "pvt_calc_local @ R%d: bufsz: %d, bufhash: %u", pctx.my_rank,
         pvt_buf_len, ::hash_str(pvt_buf, pvt_buf_len));

    flog(LOG_DBUG, "sending pivots, count: %d", pvtcnt);

    carp_->LogPrintf("RENEG_RTP_PVT_SEND");

    SendToRank(pvt_buf, pvt_buf_len, root_[1], MSGFMT_RTP_PIVOT);
  }

  return s;
}

Status RTP::HandlePivots(void* buf, unsigned int bufsz, int src) {
  Status s = Status::OK();

  int msg_round_num, stage_num, sender_id, pivot_count;
  double pivot_weight;
  double* pivots;

  flog(LOG_DBG2, "rtp_handle_reneg_pivot: bufsz: %u, bufhash, %u", bufsz,
       ::hash_str((char*)buf, bufsz));

  msgfmt_decode_rtp_pivots(buf, bufsz, &msg_round_num, &stage_num, &sender_id,
                           &pivots, &pivot_weight, &pivot_count);
  /* stage_num refers to the stage the pivots were generated at.
   * merged_pvtcnt must correspond to the next stage
   */
  int merged_pvtcnt =
      (stage_num >= 3) ? (num_ranks_ + 1) : pvtcnt_[stage_num + 1];

  assert(pivot_count <= CARP_MAXPIVOTS);

  flog(LOG_DBUG, "rtp_handle_reneg_pivot: S%d %d pivots from %d", stage_num,
       pivot_count, sender_id);

  if (pivot_count >= 4) {
    flog(LOG_DBG2, "rtp_handle_reneg_pivot: %.2f %.2f %.2f %.2f ...", pivots[0],
         pivots[1], pivots[2], pivots[3]);
  }

  mutex_.Lock();

  int stage_pivot_count = 0;

#define EXPECTED_ITEMS_FOR_STAGE(stage_idx, item_count) \
  (fanout_[(stage_idx)] == (item_count))

  if (msg_round_num != round_num_) {
    stage_pivot_count = pivot_buffer_.StoreData(
        stage_num, pivots, pivot_count, pivot_weight, /* isnextround */ true);

    /* If we're receiving a pivot for a future round, we can never have
     * received all pivots for a future round, as we also expect one pivot
     * from ourselves, which can't be received out of turn
     *
     * This assumption is important, as it enables us to not have to store
     * and replay the handler logic for when all items of a future round/stage
     * have been received
     */
    assert(!EXPECTED_ITEMS_FOR_STAGE(stage_num, stage_pivot_count));
  } else {
    stage_pivot_count = pivot_buffer_.StoreData(
        stage_num, pivots, pivot_count, pivot_weight, /*isnextround */ false);
  }
  mutex_.Unlock();

  flog(LOG_INFO,
       "rtp_handle_reneg_pivot: S%d at Rank %d, item from %d. Total: %d",
       stage_num, my_rank_, src, stage_pivot_count);

  if (EXPECTED_ITEMS_FOR_STAGE(stage_num, stage_pivot_count)) {
    reneg_bench_.MarkStageComplete(stage_num);

    double merged_pivots[merged_pvtcnt];
    double merged_weight;

    ComputeAggregatePivots(stage_num, merged_pvtcnt, merged_pivots,
                           merged_weight);

    flog(LOG_DBUG, "compute_aggr_pvts: R%d - %s - %.1f (cnt: %d)", my_rank_,
         darr2str(merged_pivots, merged_pvtcnt).c_str(),
         merged_weight, merged_pvtcnt);

    flog(LOG_INFO, "rtp_handle_reneg_pivot: S%d at Rank %d, collected",
         stage_num, my_rank_);

    size_t next_buf_sz = msgfmt_bufsize_rtp_pivots(merged_pvtcnt);
    char next_buf[next_buf_sz];

    if (stage_num < STAGES_MAX) {
      flog(LOG_DBUG, "rtp_handle_reneg_pivot: choice 1");

      // char next_buf[2048];

      int next_buf_len = msgfmt_encode_rtp_pivots(
          next_buf, next_buf_sz, msg_round_num, stage_num + 1, my_rank_,
          merged_pivots, merged_weight, merged_pvtcnt);

      int new_dest = stage_num == 1 ? root_[2] : root_[3];

      SendToRank(next_buf, next_buf_len, new_dest, MSGFMT_RTP_PIVOT);
    } else {
      /* New pivots need to be broadcast from S3. We send them back to
       * ourself (s3root) so that the broadcast code can be cleanly
       * contained in rtp_handle_pivot_bcast
       */
      flog(LOG_DBUG, "rtp_handle_reneg_pivot: choice 2 @ %d", root_[3]);
      if (merged_weight == 0.0) {
        flog(LOG_WARN, "rtp_handle_reneg_pivot: root (%d) merged pivots "
                       "has zero weight?!?!", root_[3]);
      }

      assert(my_rank_ == root_[3]);

      reneg_bench_.MarkPvtBcast();

      /* XXX: pivot_count = comm_sz, 2048B IS NOT SUFFICIENT */
      int next_buf_len = msgfmt_encode_rtp_pivots(
          next_buf, next_buf_sz, msg_round_num, stage_num + 1, my_rank_,
          merged_pivots, merged_weight, merged_pvtcnt);

      carp_->LogPrintf("RENEG_RTP_PVT_MASS %f",
                       (merged_pvtcnt - 1) * merged_weight);

      SendToRank(next_buf, next_buf_len, root_[3], MSGFMT_RTP_PVT_BCAST);
    }  // if
  }    // if

  return s;
}

Status RTP::HandlePivotBroadcast(void* buf, unsigned int bufsz, int src) {
  /* You only expect to receive this message once per-round;
   * TODO: parse the round-num for a FATAL error verification
   * but no intermediate state change necessary here. Process
   * the update and then move to READY
   */
  Status s = Status::OK();

  mutex_.Lock();

  RenegState rstate = state_.GetState();

  if (rstate != RenegState::PVTSND) {
    ABORT("rtp_handle_pivot_bcast: unexpected pivot bcast");
  }

  mutex_.Unlock();

  /* send_to_all here excludes self */
  SendToChildren(buf, bufsz, /* exclude_self */ true, MSGFMT_RTP_PVT_BCAST);

  int round_num, stage_num, sender_id, nbuf_pivots;
  double *buf_pivots, weight;
  int pivot_count = num_ranks_ + 1;
  Pivots pivots_aggr(pivot_count);

  msgfmt_decode_rtp_pivots(buf, bufsz, &round_num, &stage_num, &sender_id,
                           &buf_pivots, &weight, &nbuf_pivots);
  assert(nbuf_pivots == pivot_count);
  pivots_aggr.LoadPivots(buf_pivots, nbuf_pivots, weight);

  flog(LOG_DBUG, "rtp_handle_pivot_bcast: received pivots at %d from %d",
       my_rank_, src);

  if (my_rank_ == 0) {
//    flog(LOG_DBUG, "rtp_handle_pivot_bcast: pivots @ %d: %s (%.1f)", my_rank_,
//         darr2str(pivots, pivot_count).c_str(), pivot_weight);

    flog(LOG_INFO, "-- carp round %d completed at rank 0 --", round_num_);
  }

  /* Install pivots, Reset state, and signal back to the main thread */
  bool replay_rtp_begin_flag = false;

  /* If next round has already started, replay its messages from buffers.
   * If not, mark self as READY, and wake up Main Thread
   */
  carp_->Lock();
  mutex_.Lock();

  carp_->LogReneg(round_num_);

  flog(LOG_DBG2, "Broadcast pivot count: %d, expected %d", pivot_count,
       num_ranks_ + 1);
  assert(pivot_count == num_ranks_ + 1);

  carp_->UpdateBinsFromPivots(&pivots_aggr);

  pivot_buffer_.AdvanceRound();
  round_num_++;

  if (state_.GetNextRoundStart()) {
    /* Next round has started, keep main thread sleeping and participate */
    replay_rtp_begin_flag = true;
    state_.UpdateState(RenegState::READYBLOCK);
    carp_->UpdateState(MainThreadState::MT_REMAIN_BLOCKED);
    flog(LOG_DBUG, "[CARP] Rank %d: continuing to round %d", my_rank_,
         round_num_);
  } else {
    /* Next round has not started yet, we're READY */
    state_.UpdateState(RenegState::READY);
    carp_->UpdateState(MainThreadState::MT_READY);
    flog(LOG_DBUG, "[CARP] Rank %d: RTP finished, READY", my_rank_);
  }

  mutex_.Unlock();

  if (!replay_rtp_begin_flag) {
    /* Since no round to replay, wake up Main Thread */
    carp_->CVSignal();
  }

  carp_->Unlock();

  if (replay_rtp_begin_flag) {
    /* Not supposed to access without lock, but logging statement so ok */
    flog(LOG_DBG2, "Rank %d: continuing to round %d", my_rank_, round_num_);
    /* Send rtp_begin to self, to resume pivot send etc */
    ReplayBegin();
  }

  return s;
}

Status RTP::ReplayBegin() {
  // XXX: commented out as it makes sense, but not tested
  // mutex_.AssertHeld();
  Status s = Status::OK();

  flog(LOG_DBUG, "replay_rtp_begin: at rank %d", my_rank_);

  char buf[256];
  // TODO: round_num is correct?
  int bufsz = msgfmt_encode_rtp_begin(buf, 256, my_rank_, round_num_);
  SendToRank(buf, bufsz, my_rank_, MSGFMT_RTP_BEGIN);

  return s;
}

/*
 * ComputeAggregatePivots: we've received and buffered (in pivot_buffer_)
 * the required number of pivot msgs for the given stage of the current
 * active round.  we need to aggragate the pivots into a single set of
 * output_pivot_count pivots using the pivot union operation.   we return
 * the pivot points and the average weight of each pivot chunk (in
 * merged_pivots and merged_weight, respectively).
 */
void RTP::ComputeAggregatePivots(int stage_num, size_t output_pivot_count,
                                 double* merged_pivots, double& merged_weight) {
  std::vector<bounds_t> boundsv;
  std::vector<double> pivot_weights;

  /* get sorted set of all pivot boundaries and weights from the buffer */
  pivot_buffer_.LoadBounds(stage_num, boundsv);
  pivot_buffer_.GetPivotWeights(stage_num, pivot_weights);

  /*
   * special case: if there are no bounds points, then we have nothing
   * to union and should return a zero filled/weighted set of merged pivots.
   */
  if (boundsv.size() == 0) {
    for (size_t lcv = 0 ; lcv < output_pivot_count ; lcv++) {
      merged_pivots[lcv] = 0.0;
    }
    merged_weight = 0.0;
    return;
  }

  /*
   * next we load the bounds and weights we got from the pivot buffer
   * into a binhistogram.  we can determine the max# of bins we'll need
   * for this using the number of pivot messages we should have received
   * for this stage and the expected pivot_count of each message.
   * (each msg contributes npchunk, plus #msg-1 for cases where
   * things don't overlap).  the actual number of bins may be smaller
   * than maxbins if we have duplicate values in the pivots.
   */
  int npivotmsgs = fanout_[stage_num];
  int npchunk = pivot_buffer_.PivotCount(stage_num) - 1;
  size_t maxbins = (npchunk * npivotmsgs) + (npivotmsgs - 1);
  BinHistogram<double,float> mergedhist(maxbins);

  pivot_union(boundsv, pivot_weights, npivotmsgs, mergedhist);
  assert(mergedhist.Size() <= maxbins);  /* verify maxbins was big enough */

  /*
   * now we want to calculate a set of equal weight pivots over the
   * bins in mergedhist that we can return to the caller.
   */
  int output_npchunk = output_pivot_count - 1;
  BinHistConsumer<double,float> cons(&mergedhist);
  for (size_t lcv = 0 ; lcv < output_pivot_count ; lcv++) {
    if (lcv)
      cons.ConsumeWeightTo(cons.TotalWeight() *
                           ((output_npchunk - lcv) / (double)output_npchunk) );
    merged_pivots[lcv] = cons.CurrentValue();
  }

  merged_weight = mergedhist.GetTotalWeight() / (double) output_npchunk;

  flog(LOG_DBG2, "resampled pivot count: s%d cnt: %zu", stage_num,
       output_pivot_count);
}

}  // namespace carp
}  // namespace pdlfs
